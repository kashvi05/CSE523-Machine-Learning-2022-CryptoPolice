{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2148d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6dddba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.3:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1373d88040>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder.config(\"spark.driver.memory\",\"5g\").config(\"spark.driver.maxResultSize\", \"5g\").getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007e7726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- Asset_ID: integer (nullable = true)\n",
      " |-- Count: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- VWAP: double (nullable = true)\n",
      " |-- Target: double (nullable = true)\n",
      "\n",
      "CPU times: user 5.92 ms, sys: 5.85 ms, total: 11.8 ms\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "X_train = spark.read.csv(\"./Dataset/train.csv\", header=True, inferSchema=True)\n",
    "X_test = spark.read.csv(\"./Dataset/example_test.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert VWAP Feature to double\n",
    "X_train = X_train.withColumn(\"VWAP\", X_train.VWAP.cast(DoubleType()))\n",
    "\n",
    "X_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769298f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(timestamp=1514764860, Asset_ID=2, Count=40.0, Open=2376.58, High=2399.5, Low=2357.14, Close=2374.59, Volume=19.23300519, VWAP=2373.1163915061647, Target=-0.004218152387429286)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "323ec537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+----+----+---+-----+------+----+------+\n",
      "|timestamp|Asset_ID|Count|Open|High|Low|Close|Volume|VWAP|Target|\n",
      "+---------+--------+-----+----+----+---+-----+------+----+------+\n",
      "|        0|       0|    0|   0|   0|  0|    0|     0|   0|     0|\n",
      "+---------+--------+-----+----+----+---+-----+------+----+------+\n",
      "\n",
      "CPU times: user 13.2 ms, sys: 6.29 ms, total: 19.5 ms\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Check null values if any\n",
    "\n",
    "X_train.select([f.count(f.when(f.isnull(c), c)).alias(c) for c in X_train.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4907b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "X_train = X_train.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e98180",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_features = ['Count', \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"VWAP\", \"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3cda5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=col_features, outputCol=\"Independent Features\")\n",
    "output = vecAssembler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9505e4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Independent Features=DenseVector([40.0, 2376.58, 2399.5, 2357.14, 2374.59, 19.233, 2373.1164, -0.0042]), Target=-0.004218152387429286),\n",
       " Row(Independent Features=DenseVector([5.0, 8.53, 8.53, 8.53, 8.53, 78.38, 8.53, -0.0144]), Target=-0.014398966468964769),\n",
       " Row(Independent Features=DenseVector([229.0, 13835.194, 14013.8, 13666.11, 13850.176, 31.5501, 13827.0621, -0.0146]), Target=-0.014643224355736173)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final data\n",
    "final_data = output.select([\"Independent Features\", \"Target\"])\n",
    "final_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c46dfaed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base Model (Without Time series considerations)\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data, test_data = final_data.randomSplit([0.75, 0.25]) # Randomly split the data into training and testing\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"Independent Features\", labelCol = \"Target\")\n",
    "\n",
    "lrModel = lr.fit(train_data) # Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b86649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e9316fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sameep/.local/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|Independent Features|              Target|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|[1.0,0.029101,0.0...|0.010354317165462756|       NaN|\n",
      "|[1.0,0.0294839999...|0.001288878336668...|       NaN|\n",
      "|[1.0,0.0303899999...|-0.01523387910549...|       NaN|\n",
      "|[1.0,0.03055,0.03...|0.007245901639344243|       NaN|\n",
      "|[1.0,0.0309,0.030...|0.013009708737864223|       NaN|\n",
      "|[1.0,0.0310300000...|-0.04964539007092...|       NaN|\n",
      "|[1.0,0.03184,0.03...|0.026107694238734824|       NaN|\n",
      "|[1.0,0.031863,0.0...|-0.03131850923896007|       NaN|\n",
      "|[1.0,0.03193,0.03...|-0.03343749999999979|       NaN|\n",
      "|[1.0,0.03199,0.03...|-0.04473272897780567|       NaN|\n",
      "|[1.0,0.032,0.032,...|-0.03998124413879...|       NaN|\n",
      "|[1.0,0.0320170000...| 0.00824561951463254|       NaN|\n",
      "|[1.0,0.032297,0.0...|-0.00673202208847...|       NaN|\n",
      "|[1.0,0.032474,0.0...|0.005145053310190...|       NaN|\n",
      "|[1.0,0.03258,0.03...|-6.09942055504686...|       NaN|\n",
      "|[1.0,0.032667,0.0...|-0.00701150132234...|       NaN|\n",
      "|[1.0,0.032761,0.0...|0.025521620804354272|       NaN|\n",
      "|[1.0,0.0329,0.032...|-0.00274390243902...|       NaN|\n",
      "|[1.0,0.032985,0.0...|-0.01825377346913...|       NaN|\n",
      "|[1.0,0.033,0.033,...|0.007018614586512095|       NaN|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 9.94 ms, sys: 6.33 ms, total: 16.3 ms\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Predict/Evaluate on test data\n",
    "\n",
    "pred_results = lrModel.evaluate(test_data)\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891d4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
